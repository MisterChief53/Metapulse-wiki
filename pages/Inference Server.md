## Setup Inference Server for Development
	- install the `DevContainers` extension on VSCode.
	- Open the folder in a container using the extension.
	- Wait a bit for it to load. Now, you have the environment in a Docker container. You are able to start new terminals that will have all dependencies,
- ## State of the LLM
	- Right now, the LLM is running on Mistral7B finetuned for instructions, and provided context for the character being a bartender.
	- Google colab document link: https://colab.research.google.com/drive/18-B3-4522mxzXLmhdfsZxz7L5AMTgVMc?hl=es%2F#scrollTo=WBqjvm7cT6vY